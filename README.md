

Synapse Mobile â€“ Backend (Python / FastAPI)

This is the backend service for Synapse Mobile, a multilingual AI chatbot application that supports:

ğŸ§  LLM responses using Grok / Llama / custom models

ğŸ¤ Speech-to-Text (STT) with Google / Vosk / OpenAI Whisper (configurable)

ğŸ”Š Text-to-Speech (TTS) with 11Labs / Edge / OpenAI (configurable)

ğŸŒ Multilingual support

ğŸŒ Web search + returns visited URLs

ğŸ”Œ REST API endpoints for the Flutter mobile app


The backend is built with FastAPI, fully stateless, and deployable on Render, Railway, Vercel, or any Linux server.


---

ğŸ”§ Project Structure

mobile-backend/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ deps.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â”œâ”€â”€ translate.py
â”‚   â”œâ”€â”€ websearch.py
â”‚   â”œâ”€â”€ stt.py
â”‚   â”œâ”€â”€ tts.py
â”‚   â”œâ”€â”€ router_chat.py
â”‚   â””â”€â”€ router_utils.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md


---

ğŸš€ Running Locally

1. Install dependencies

pip install -r requirements.txt

2. Copy .env.example to .env and add your keys

GROK_API_KEY=your_key_here
ELEVENLABS_API_KEY=your_key_here

3. Start server

uvicorn app.main:app --reload

Backend runs at:

http://localhost:8000


---

ğŸŒ Deployment

This backend is optimized for:

Render (recommended)

Railway

Fly.io

Any VM / Docker server


Just connect your GitHub repository and deploy.


---

ğŸ“¡ Endpoints (Summary)

Method	URL	Description

POST	/chat/	LLM chatbot response
POST	/stt/	Speech to Text
POST	/tts/	Text to Speech
GET	/search?q=	Web search
GET	/health	Status check



---

ğŸ“¦ Tech Stack

FastAPI â€“ Backend framework

Grok / Llama API â€“ LLM

11Labs / Edge / Whisper â€“ TTS / STT

DuckDuckGo / SerpAPI â€“ Web search

Python 3.11+



---

ğŸ“ License

This project currently uses no license (private).